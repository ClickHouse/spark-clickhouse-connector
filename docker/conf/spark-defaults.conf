#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

spark.driver.host=0.0.0.0
spark.ui.port=4040
spark.sql.shuffle.partitions=16
spark.sql.warehouse.dir=s3a://spark-bucket/warehouse
spark.sql.defaultCatalog=spark_catalog
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.hadoop.fs.s3a.bucket.spark-bucket.committer.magic.enabled=true
spark.hadoop.fs.s3a.bucket.iceberg-bucket.committer.magic.enabled=true

spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type=hive

spark.sql.catalog.tpcds=org.apache.kyuubi.spark.connector.tpcds.TPCDSCatalog
spark.sql.catalog.tpch=org.apache.kyuubi.spark.connector.tpch.TPCHCatalog

spark.sql.catalog.postgres=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog
spark.sql.catalog.postgres.url=jdbc:postgresql://postgres:5432/metastore
spark.sql.catalog.postgres.driver=org.postgresql.Driver
spark.sql.catalog.postgres.user=postgres
spark.sql.catalog.postgres.password=postgres

spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type=hadoop
spark.sql.catalog.iceberg.warehouse=s3a://iceberg-bucket/iceberg-warehouse

spark.sql.catalog.clickhouse_s1r1=com.clickhouse.spark.ClickHouseCatalog
spark.sql.catalog.clickhouse_s1r1.host=clickhouse-s1r1
spark.sql.catalog.clickhouse_s1r1.protocol=http
spark.sql.catalog.clickhouse_s1r1.http_port=8123
spark.sql.catalog.clickhouse_s1r1.user=default
spark.sql.catalog.clickhouse_s1r1.password=
spark.sql.catalog.clickhouse_s1r1.database=default
spark.sql.catalog.clickhouse_s1r1.option.custom_http_params=async_insert=1,wait_for_async_insert=1

spark.sql.catalog.clickhouse_s1r2=com.clickhouse.spark.ClickHouseCatalog
spark.sql.catalog.clickhouse_s1r2.host=clickhouse-s1r2
spark.sql.catalog.clickhouse_s1r2.protocol=http
spark.sql.catalog.clickhouse_s1r2.http_port=8123
spark.sql.catalog.clickhouse_s1r2.user=default
spark.sql.catalog.clickhouse_s1r2.password=
spark.sql.catalog.clickhouse_s1r2.database=default
spark.sql.catalog.clickhouse_s1r2.option.custom_http_params=async_insert=1,wait_for_async_insert=1

spark.sql.catalog.clickhouse_s2r1=com.clickhouse.spark.ClickHouseCatalog
spark.sql.catalog.clickhouse_s2r1.host=clickhouse-s2r1
spark.sql.catalog.clickhouse_s2r1.protocol=http
spark.sql.catalog.clickhouse_s2r1.http_port=8123
spark.sql.catalog.clickhouse_s2r1.user=default
spark.sql.catalog.clickhouse_s2r1.password=
spark.sql.catalog.clickhouse_s2r1.database=default
spark.sql.catalog.clickhouse_s2r1.option.custom_http_params=async_insert=1,wait_for_async_insert=1

spark.sql.catalog.clickhouse_s2r2=com.clickhouse.spark.ClickHouseCatalog
spark.sql.catalog.clickhouse_s2r2.host=clickhouse-s2r2
spark.sql.catalog.clickhouse_s2r2.protocol=http
spark.sql.catalog.clickhouse_s2r2.http_port=8123
spark.sql.catalog.clickhouse_s2r2.user=default
spark.sql.catalog.clickhouse_s2r2.password=
spark.sql.catalog.clickhouse_s2r2.database=default
spark.sql.catalog.clickhouse_s2r2.option.custom_http_params=async_insert=1,wait_for_async_insert=1

spark.clickhouse.write.batchSize=50000
spark.clickhouse.write.maxRetry=2
spark.clickhouse.write.retryInterval=1
spark.clickhouse.write.retryableErrorCodes=241
spark.clickhouse.write.write.repartitionNum=0
spark.clickhouse.write.distributed.useClusterNodes=true
spark.clickhouse.read.distributed.useClusterNodes=false
spark.clickhouse.write.distributed.convertLocal=true
spark.clickhouse.read.distributed.convertLocal=true
