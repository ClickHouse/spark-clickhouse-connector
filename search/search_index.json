{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Spark ClickHouse Connector is a high performance connector build on top of Spark DataSource V2.</p> <p></p>"},{"location":"#requirements","title":"Requirements","text":"<ol> <li>Basic knowledge of Apache Spark and ClickHouse.</li> <li>An available ClickHouse single node or cluster.</li> <li>An available Spark cluster, check the following Compatible Matrix to make sure the Spark version is compatible with this Connector.</li> <li>Make sure your network policy satisfies the following requirements, both driver and executor of Spark need to access     ClickHouse HTTP port. If you are using it to access ClickHouse cluster, ensure the connectivity between driver and    executor of Spark and each node of ClickHouse cluster.</li> </ol>"},{"location":"#notes","title":"Notes","text":"<ol> <li>Integration tests based on Java 8 &amp; 17, Scala 2.12 &amp; 2.13, Spark 3.4 and ClickHouse    v23.8, with both single ClickHouse instance and ClickHouse cluster.</li> </ol>"},{"location":"#compatible-matrix","title":"Compatible Matrix","text":"<p>For old versions, please refer the compatible matrix.</p> Version Compatible Spark Versions ClickHouse JDBC version master Spark 3.3, 3.4, 3.5 0.6.0 0.7.3 Spark 3.3, 3.4 0.4.6 0.6.0 Spark 3.3 0.3.2-patch11 0.5.0 Spark 3.2, 3.3 0.3.2-patch11 0.4.0 Spark 3.2, 3.3 Not depend on 0.3.0 Spark 3.2, 3.3 Not depend on 0.2.1 Spark 3.2 Not depend on 0.1.2 Spark 3.2 Not depend on"},{"location":"best_practices/","title":"TODO","text":""},{"location":"best_practices/01_deployment/","title":"Deployment","text":""},{"location":"best_practices/01_deployment/#jar","title":"Jar","text":"<p>Put <code>clickhouse-spark-runtime-3.4_2.12-0.7.3.jar</code> and <code>clickhouse-jdbc-0.4.6-all.jar</code> into <code>$SPARK_HOME/jars/</code>, then you don't need to bundle the jar into your Spark application, and <code>--jar</code> is not required when using <code>spark-shell</code> or <code>spark-sql</code>(again, for SQL-only use cases, Apache Kyuubi is recommended for Production).</p>"},{"location":"best_practices/01_deployment/#configuration","title":"Configuration","text":"<p>Persist catalog configurations into <code>$SPARK_HOME/conf/spark-defaults.conf</code>, then <code>--conf</code>s are not required when using <code>spark-shell</code> or <code>spark-sql</code>.</p> <pre><code>spark.sql.catalog.ck_01=xenon.clickhouse.ClickHouseCatalog\nspark.sql.catalog.ck_01.host=10.0.0.1\nspark.sql.catalog.ck_01.protocol=http\nspark.sql.catalog.ck_01.http_port=8123\nspark.sql.catalog.ck_01.user=app\nspark.sql.catalog.ck_01.password=pwd\nspark.sql.catalog.ck_01.database=default\n\nspark.sql.catalog.ck_02=xenon.clickhouse.ClickHouseCatalog\nspark.sql.catalog.ck_02.host=10.0.0.2\nspark.sql.catalog.ck_02.protocol=http\nspark.sql.catalog.ck_02.http_port=8123\nspark.sql.catalog.ck_02.user=app\nspark.sql.catalog.ck_02.password=pwd\nspark.sql.catalog.ck_02.database=default\n</code></pre>"},{"location":"configurations/","title":"Configurations","text":""},{"location":"configurations/#catalog-configurations","title":"Catalog Configurations","text":""},{"location":"configurations/#single-instance","title":"Single Instance","text":"<p>Suppose you have one ClickHouse instance which installed on <code>10.0.0.1</code> and exposes HTTP endpoint on <code>8123</code>.</p> <p>Edit <code>$SPARK_HOME/conf/spark-defaults.conf</code>.</p> <pre><code>####################################################################################\n## register a catalog named \"clickhouse\"\n####################################################################################\nspark.sql.catalog.clickhouse                      xenon.clickhouse.ClickHouseCatalog\n\n####################################################################################\n## basic configurations for \"clickhouse\" catalog\n####################################################################################\nspark.sql.catalog.clickhouse.host                 10.0.0.1\nspark.sql.catalog.clickhouse.protocol             http\nspark.sql.catalog.clickhouse.http_port            8123\nspark.sql.catalog.clickhouse.user                 default\nspark.sql.catalog.clickhouse.password\nspark.sql.catalog.clickhouse.database             default\n\n##############################################################################################\n## custom options of clickhouse-client for \"clickhouse\" catalog\n##############################################################################################\nspark.sql.catalog.clickhouse.option.ssl                 false\nspark.sql.catalog.clickhouse.option.async               false\nspark.sql.catalog.clickhouse.option.client_name         spark\nspark.sql.catalog.clickhouse.option.custom_http_params  async_insert=1,wait_for_async_insert=1\n</code></pre> <p>Then you can access ClickHouse table <code>&lt;ck_db&gt;.&lt;ck_table&gt;</code> from Spark SQL by using <code>clickhouse.&lt;ck_db&gt;.&lt;ck_table&gt;</code>.</p>"},{"location":"configurations/#cluster","title":"Cluster","text":"<p>For ClickHouse cluster, give an unique catalog name for each instances.</p> <p>Suppose you have two ClickHouse instances, one installed on <code>10.0.0.1</code> and exposes HTTPS endpoint on port <code>8443</code> named clickhouse1, and another installed on <code>10.0.0.2</code> and exposes HTTPS endpoint on port <code>8443</code> named clickhouse2.</p> <p>Edit <code>$SPARK_HOME/conf/spark-defaults.conf</code>.</p> <pre><code>spark.sql.catalog.clickhouse1                xenon.clickhouse.ClickHouseCatalog\nspark.sql.catalog.clickhouse1.host           10.0.0.1\nspark.sql.catalog.clickhouse1.protocol       https\nspark.sql.catalog.clickhouse1.http_port      8443\nspark.sql.catalog.clickhouse1.user           default\nspark.sql.catalog.clickhouse1.password\nspark.sql.catalog.clickhouse1.database       default\nspark.sql.catalog.clickhouse1.option.ssl     true\n\nspark.sql.catalog.clickhouse2                xenon.clickhouse.ClickHouseCatalog\nspark.sql.catalog.clickhouse2.host           10.0.0.2\nspark.sql.catalog.clickhouse2.protocol       https\nspark.sql.catalog.clickhouse2.http_port      8443\nspark.sql.catalog.clickhouse2.user           default\nspark.sql.catalog.clickhouse2.password\nspark.sql.catalog.clickhouse2.database       default\nspark.sql.catalog.clickhouse2.option.ssl     true\n</code></pre> <p>Then you can access clickhouse1 table <code>&lt;ck_db&gt;.&lt;ck_table&gt;</code> from Spark SQL by <code>clickhouse1.&lt;ck_db&gt;.&lt;ck_table&gt;</code>, and access clickhouse2 table <code>&lt;ck_db&gt;.&lt;ck_table&gt;</code> by <code>clickhouse2.&lt;ck_db&gt;.&lt;ck_table&gt;</code>.</p>"},{"location":"configurations/#sql-configurations","title":"SQL Configurations","text":"<p>SQL Configurations could be overwritten by <code>SET &lt;key&gt;=&lt;value&gt;</code> in runtime.</p> Key Default Description Since spark.clickhouse.ignoreUnsupportedTransform false ClickHouse supports using complex expressions as sharding keys or partition values, e.g. <code>cityHash64(col_1, col_2)</code>, and those can not be supported by Spark now. If <code>true</code>, ignore the unsupported expressions, otherwise fail fast w/ an exception. Note, when <code>spark.clickhouse.write.distributed.convertLocal</code> is enabled, ignore unsupported sharding keys may corrupt the data. 0.4.0 spark.clickhouse.read.compression.codec lz4 The codec used to decompress data for reading. Supported codecs: none, lz4. 0.5.0 spark.clickhouse.read.distributed.convertLocal true When reading Distributed table, read local table instead of itself. If <code>true</code>, ignore <code>spark.clickhouse.read.distributed.useClusterNodes</code>. 0.1.0 spark.clickhouse.read.fixedStringAs binary Read ClickHouse FixedString type as the specified Spark data type. Supported types: binary, string 0.8.0 spark.clickhouse.read.format json Serialize format for reading. Supported formats: json, binary 0.6.0 spark.clickhouse.read.runtimeFilter.enabled false Enable runtime filter for reading. 0.8.0 spark.clickhouse.read.splitByPartitionId true If <code>true</code>, construct input partition filter by virtual column <code>_partition_id</code>, instead of partition value. There are known bugs to assemble SQL predication by partition value. This feature requires ClickHouse Server v21.6+ 0.4.0 spark.clickhouse.useNullableQuerySchema false If <code>true</code>, mark all the fields of the query schema as nullable when executing <code>CREATE/REPLACE TABLE ... AS SELECT ...</code> on creating the table. Note, this configuration requires SPARK-43390(available in Spark 3.5), w/o this patch, it always acts as <code>true</code>. 0.8.0 spark.clickhouse.write.batchSize 10000 The number of records per batch on writing to ClickHouse. 0.1.0 spark.clickhouse.write.compression.codec lz4 The codec used to compress data for writing. Supported codecs: none, lz4. 0.3.0 spark.clickhouse.write.distributed.convertLocal false When writing Distributed table, write local table instead of itself. If <code>true</code>, ignore <code>spark.clickhouse.write.distributed.useClusterNodes</code>. 0.1.0 spark.clickhouse.write.distributed.useClusterNodes true Write to all nodes of cluster when writing Distributed table. 0.1.0 spark.clickhouse.write.format arrow Serialize format for writing. Supported formats: json, arrow 0.4.0 spark.clickhouse.write.localSortByKey true If <code>true</code>, do local sort by sort keys before writing. 0.3.0 spark.clickhouse.write.localSortByPartition If <code>true</code>, do local sort by partition before writing. If not set, it equals to <code>spark.clickhouse.write.repartitionByPartition</code>. 0.3.0 spark.clickhouse.write.maxRetry 3 The maximum number of write we will retry for a single batch write failed with retryable codes. 0.1.0 spark.clickhouse.write.repartitionByPartition true Whether to repartition data by ClickHouse partition keys to meet the distributions of ClickHouse table before writing. 0.3.0 spark.clickhouse.write.repartitionNum 0 Repartition data to meet the distributions of ClickHouse table is required before writing, use this conf to specific the repartition number, value less than 1 mean no requirement. 0.1.0 spark.clickhouse.write.repartitionStrictly false If <code>true</code>, Spark will strictly distribute incoming records across partitions to satisfy the required distribution before passing the records to the data source table on write. Otherwise, Spark may apply certain optimizations to speed up the query but break the distribution requirement. Note, this configuration requires SPARK-37523(available in Spark 3.4), w/o this patch, it always acts as <code>true</code>. 0.3.0 spark.clickhouse.write.retryInterval 10s The interval in seconds between write retry. 0.1.0 spark.clickhouse.write.retryableErrorCodes 241 The retryable error codes returned by ClickHouse server when write failing. 0.1.0"},{"location":"configurations/01_catalog_configurations/","title":"01 catalog configurations","text":""},{"location":"configurations/01_catalog_configurations/#single-instance","title":"Single Instance","text":"<p>Suppose you have one ClickHouse instance which installed on <code>10.0.0.1</code> and exposes HTTP endpoint on <code>8123</code>.</p> <p>Edit <code>$SPARK_HOME/conf/spark-defaults.conf</code>.</p> <pre><code>####################################################################################\n## register a catalog named \"clickhouse\"\n####################################################################################\nspark.sql.catalog.clickhouse                      xenon.clickhouse.ClickHouseCatalog\n\n####################################################################################\n## basic configurations for \"clickhouse\" catalog\n####################################################################################\nspark.sql.catalog.clickhouse.host                 10.0.0.1\nspark.sql.catalog.clickhouse.protocol             http\nspark.sql.catalog.clickhouse.http_port            8123\nspark.sql.catalog.clickhouse.user                 default\nspark.sql.catalog.clickhouse.password\nspark.sql.catalog.clickhouse.database             default\n\n##############################################################################################\n## custom options of clickhouse-client for \"clickhouse\" catalog\n##############################################################################################\nspark.sql.catalog.clickhouse.option.ssl                 false\nspark.sql.catalog.clickhouse.option.async               false\nspark.sql.catalog.clickhouse.option.client_name         spark\nspark.sql.catalog.clickhouse.option.custom_http_params  async_insert=1,wait_for_async_insert=1\n</code></pre> <p>Then you can access ClickHouse table <code>&lt;ck_db&gt;.&lt;ck_table&gt;</code> from Spark SQL by using <code>clickhouse.&lt;ck_db&gt;.&lt;ck_table&gt;</code>.</p>"},{"location":"configurations/01_catalog_configurations/#cluster","title":"Cluster","text":"<p>For ClickHouse cluster, give an unique catalog name for each instances.</p> <p>Suppose you have two ClickHouse instances, one installed on <code>10.0.0.1</code> and exposes HTTPS endpoint on port <code>8443</code> named clickhouse1, and another installed on <code>10.0.0.2</code> and exposes HTTPS endpoint on port <code>8443</code> named clickhouse2.</p> <p>Edit <code>$SPARK_HOME/conf/spark-defaults.conf</code>.</p> <pre><code>spark.sql.catalog.clickhouse1                xenon.clickhouse.ClickHouseCatalog\nspark.sql.catalog.clickhouse1.host           10.0.0.1\nspark.sql.catalog.clickhouse1.protocol       https\nspark.sql.catalog.clickhouse1.http_port      8443\nspark.sql.catalog.clickhouse1.user           default\nspark.sql.catalog.clickhouse1.password\nspark.sql.catalog.clickhouse1.database       default\nspark.sql.catalog.clickhouse1.option.ssl     true\n\nspark.sql.catalog.clickhouse2                xenon.clickhouse.ClickHouseCatalog\nspark.sql.catalog.clickhouse2.host           10.0.0.2\nspark.sql.catalog.clickhouse2.protocol       https\nspark.sql.catalog.clickhouse2.http_port      8443\nspark.sql.catalog.clickhouse2.user           default\nspark.sql.catalog.clickhouse2.password\nspark.sql.catalog.clickhouse2.database       default\nspark.sql.catalog.clickhouse2.option.ssl     true\n</code></pre> <p>Then you can access clickhouse1 table <code>&lt;ck_db&gt;.&lt;ck_table&gt;</code> from Spark SQL by <code>clickhouse1.&lt;ck_db&gt;.&lt;ck_table&gt;</code>, and access clickhouse2 table <code>&lt;ck_db&gt;.&lt;ck_table&gt;</code> by <code>clickhouse2.&lt;ck_db&gt;.&lt;ck_table&gt;</code>.</p>"},{"location":"configurations/02_sql_configurations/","title":"02 sql configurations","text":"Key Default Description Since spark.clickhouse.ignoreUnsupportedTransform false ClickHouse supports using complex expressions as sharding keys or partition values, e.g. <code>cityHash64(col_1, col_2)</code>, and those can not be supported by Spark now. If <code>true</code>, ignore the unsupported expressions, otherwise fail fast w/ an exception. Note, when <code>spark.clickhouse.write.distributed.convertLocal</code> is enabled, ignore unsupported sharding keys may corrupt the data. 0.4.0 spark.clickhouse.read.compression.codec lz4 The codec used to decompress data for reading. Supported codecs: none, lz4. 0.5.0 spark.clickhouse.read.distributed.convertLocal true When reading Distributed table, read local table instead of itself. If <code>true</code>, ignore <code>spark.clickhouse.read.distributed.useClusterNodes</code>. 0.1.0 spark.clickhouse.read.fixedStringAs binary Read ClickHouse FixedString type as the specified Spark data type. Supported types: binary, string 0.8.0 spark.clickhouse.read.format json Serialize format for reading. Supported formats: json, binary 0.6.0 spark.clickhouse.read.runtimeFilter.enabled false Enable runtime filter for reading. 0.8.0 spark.clickhouse.read.splitByPartitionId true If <code>true</code>, construct input partition filter by virtual column <code>_partition_id</code>, instead of partition value. There are known bugs to assemble SQL predication by partition value. This feature requires ClickHouse Server v21.6+ 0.4.0 spark.clickhouse.useNullableQuerySchema false If <code>true</code>, mark all the fields of the query schema as nullable when executing <code>CREATE/REPLACE TABLE ... AS SELECT ...</code> on creating the table. Note, this configuration requires SPARK-43390(available in Spark 3.5), w/o this patch, it always acts as <code>true</code>. 0.8.0 spark.clickhouse.write.batchSize 10000 The number of records per batch on writing to ClickHouse. 0.1.0 spark.clickhouse.write.compression.codec lz4 The codec used to compress data for writing. Supported codecs: none, lz4. 0.3.0 spark.clickhouse.write.distributed.convertLocal false When writing Distributed table, write local table instead of itself. If <code>true</code>, ignore <code>spark.clickhouse.write.distributed.useClusterNodes</code>. 0.1.0 spark.clickhouse.write.distributed.useClusterNodes true Write to all nodes of cluster when writing Distributed table. 0.1.0 spark.clickhouse.write.format arrow Serialize format for writing. Supported formats: json, arrow 0.4.0 spark.clickhouse.write.localSortByKey true If <code>true</code>, do local sort by sort keys before writing. 0.3.0 spark.clickhouse.write.localSortByPartition If <code>true</code>, do local sort by partition before writing. If not set, it equals to <code>spark.clickhouse.write.repartitionByPartition</code>. 0.3.0 spark.clickhouse.write.maxRetry 3 The maximum number of write we will retry for a single batch write failed with retryable codes. 0.1.0 spark.clickhouse.write.repartitionByPartition true Whether to repartition data by ClickHouse partition keys to meet the distributions of ClickHouse table before writing. 0.3.0 spark.clickhouse.write.repartitionNum 0 Repartition data to meet the distributions of ClickHouse table is required before writing, use this conf to specific the repartition number, value less than 1 mean no requirement. 0.1.0 spark.clickhouse.write.repartitionStrictly false If <code>true</code>, Spark will strictly distribute incoming records across partitions to satisfy the required distribution before passing the records to the data source table on write. Otherwise, Spark may apply certain optimizations to speed up the query but break the distribution requirement. Note, this configuration requires SPARK-37523(available in Spark 3.4), w/o this patch, it always acts as <code>true</code>. 0.3.0 spark.clickhouse.write.retryInterval 10s The interval in seconds between write retry. 0.1.0 spark.clickhouse.write.retryableErrorCodes 241 The retryable error codes returned by ClickHouse server when write failing. 0.1.0"},{"location":"developers/","title":"TODO","text":""},{"location":"developers/01_build_and_test/","title":"Build and Test","text":""},{"location":"developers/01_build_and_test/#build","title":"Build","text":"<p>Check out source code from GitHub</p> <pre><code>git checkout https://github.com/housepower/spark-clickhouse-connector.git\n</code></pre> <p>Build w/o test</p> <pre><code>./gradlew clean build -x test\n</code></pre> <p>Go to <code>spark-3.4/clickhouse-spark-runtime/build/libs/</code> to find the output jar <code>clickhouse-spark-runtime-3.4_2.12-0.8.0-SNAPSHOT.jar</code>.</p>"},{"location":"developers/01_build_and_test/#test","title":"Test","text":"<p>The project leverage Testcontainers and Docker Compose to do integration tests, you should install Docker and Docker Compose before running test, and check more details on Testcontainers document if you'd like to run test with remote Docker daemon.</p> <p>Run all test</p> <p><code>./gradlew clean test</code></p> <p>Run single test</p> <p><code>./gradlew test --tests=ConvertDistToLocalWriteSuite</code></p> <p>Test against custom ClickHouse image</p> <p><code>CLICKHOUSE_IMAGE=custom-org/clickhouse-server:custom-tag ./gradlew test</code></p>"},{"location":"developers/02_docs_and_website/","title":"Docs and Website","text":""},{"location":"developers/02_docs_and_website/#setup-python","title":"Setup Python","text":"<p>Follow the Python official document to install.</p>"},{"location":"developers/02_docs_and_website/#setup-pyenv-on-macos-optional","title":"Setup <code>pyenv</code> on macOS (optional)","text":"<p>Optionally, recommend to manage Python environments by pyenv.</p> <p>Install from Homebrew</p> <pre><code>brew install pyenv pyenv-virtualenv\n</code></pre> <p>Setup in <code>~/.zshrc</code></p> <pre><code>eval \"$(pyenv init -)\"\neval \"$(pyenv virtualenv-init -)\"\n</code></pre> <p>Install <code>virtualenv</code></p> <pre><code>pyenv install 3.9.13\npyenv virtualenv 3.9.13 scc\n</code></pre> <p>Localize <code>virtualenv</code></p> <pre><code>pyenv local scc\n</code></pre>"},{"location":"developers/02_docs_and_website/#install-dependencies","title":"Install dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"developers/02_docs_and_website/#preview-website","title":"Preview website","text":"<pre><code>mkdocs serve\n</code></pre> <p>Open http://127.0.0.1:8000/ in browser.</p>"},{"location":"developers/03_private_release/","title":"Private Release","text":"<p>Tip</p> <p>Internal Release means deploying to private Nexus Repository. Please make sure you are granted to access your company private Nexus Repository.</p>"},{"location":"developers/03_private_release/#repository-and-authentication","title":"Repository and Authentication","text":"<p>Configure Gradle in <code>~/.gradle/gradle.properties</code>.</p> <pre><code>mavenUser=xxx\nmavenPassword=xxx\nmavenReleasesRepo=xxx\nmavenSnapshotsRepo=xxx\n</code></pre>"},{"location":"developers/03_private_release/#upgrade-version","title":"Upgrade Version","text":"<p>Modify version in <code>version.txt</code> and <code>docker/.env-dev</code></p>"},{"location":"developers/03_private_release/#build-and-deploy","title":"Build and Deploy","text":"<p>Publish to Maven Repository using <code>./gradlew publish</code></p>"},{"location":"developers/04_public_release/","title":"Public Release","text":"<p>Notice</p> <p>Public Release means deploying to Maven Central. Only core team members are granted to deploy into Public Repository.</p> <p>Note</p> <p>Most of the steps for a public release are done by the GitHub workflow.</p>"},{"location":"developers/04_public_release/#snapshot-release","title":"Snapshot Release","text":"<p>The daily snapshot release is managed by Publish Snapshot workflow, it is scheduled to be deployed at midnight every day.</p>"},{"location":"developers/04_public_release/#feature-release","title":"Feature Release","text":"<ol> <li>Cut new branch from master branch, e.g. <code>branch-0.3</code>;</li> <li>Update version in <code>version.txt</code> and <code>docker/.env-dev</code>, e.g. from <code>0.3.0-SNAPSHOT</code> to <code>0.3.0</code>;</li> <li>Create new tag, e.g. <code>v0.3.0</code>, it will trigger the Publish Release    workflow; </li> <li>Verify, close, and release in Sonatype Repository</li> <li>Announce in GitHub Release</li> <li>Update version in <code>version.txt</code> and <code>docker/.env-dev</code>, e.g. from <code>0.3.0</code> to <code>0.3.1-SNAPSHOT</code>;</li> <li>Update version on master branch in <code>version.txt</code> and <code>docker/.env-dev</code>, e.g. from <code>0.3.0-SNAPSHOT</code> to <code>0.4.0-SNAPSHOT</code>;</li> <li>Publish Docker image after jars    available in Maven Central, generally it costs few minutes after step 3.</li> </ol>"},{"location":"developers/04_public_release/#patch-release","title":"Patch Release","text":"<p>Just emit step 1 and step 7 from feature release.</p>"},{"location":"internals/","title":"Overview Design","text":"<p>In high level, Spark ClickHouse Connector is a connector build on top of Spark DataSource V2 and ClickHouse HTTP protocol.</p> <p></p>"},{"location":"internals/01_catalog/","title":"Catalog Management","text":"<p>One important end user facing feature of DataSource V2 is supporting of multi-catalogs.</p> <p>In the early stage of Spark, it does not have catalog concept, usually, user uses Hive Metastore or Glue to manage table metadata, hence user must register external DataSource tables in the centralized metastore before accessing.</p> <p>In the centralized metastore model, a table is identified by <code>&lt;database&gt;.&lt;table&gt;</code>.</p> <p></p> <p>For example, register a MySQL table into metastore, then access it using Spark SQL.</p> <pre><code>CREATE TABLE &lt;db&gt;.&lt;tbl&gt;\nUSING org.apache.spark.sql.jdbc\nOPTIONS (\n  url      \"jdbc:mysql://&lt;mysql_host&gt;:&lt;mysql_port&gt;\",\n  dbtable  \"&lt;mysql_db&gt;.&lt;mysql_tbl&gt;\",\n  user     \"&lt;mysql_username&gt;\",\n  password \"&lt;mysql_password&gt;\"\n);\n</code></pre> <pre><code>SELECT * FROM &lt;db&gt;.&lt;tbl&gt;;\nINSERT INTO &lt;db&gt;.&lt;tbl&gt; SELECT ...\n</code></pre> <p>Things changed in DataSource V2, starting from Spark 3.0, catalog concept is introduced to allow Spark to discover tables automatically by registering catalog plugins.</p> <p>The default catalog has a fixed name <code>spark_catalog</code>, and typically, a table is identified by <code>&lt;catalog&gt;.&lt;database&gt;.&lt;table&gt;</code>.</p> <p></p> <p>For example, we can register a PostgreSQL database as Spark catalog named <code>pg</code>, and access it using Spark SQL.</p> <pre><code># spark-defaults.conf\nspark.sql.catalog.pg=org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog\nspark.sql.catalog.pg.url=jdbc:postgresql://&lt;pg_host&gt;:&lt;pg_host&gt;/&lt;pg_db&gt;\nspark.sql.catalog.pg.driver=org.postgresql.Driver\nspark.sql.catalog.pg.user=&lt;pg_username&gt;\nspark.sql.catalog.pg.password=&lt;pg_password&gt;\n</code></pre> <pre><code>SELECT * FROM pg.&lt;db&gt;.&lt;tbl&gt;;\nINSERT INTO pg.&lt;db&gt;.&lt;tbl&gt; SELECT ...\n</code></pre>"},{"location":"internals/02_read/","title":"How reading of the connector works?","text":""},{"location":"internals/02_read/#push-down","title":"Push Down","text":"<p>Spark supports push down the processing of queries, or parts of queries, into the connected data source. This means that a specific predicate, aggregation function, or other operation, could be passed through to ClickHouse for processing.</p> <p>The results of this push down can include the following benefits:</p> <ul> <li> <p>Improved overall query performance</p> </li> <li> <p>Reduced network traffic between Spark and ClickHouse</p> </li> <li> <p>Reduced load on ClickHouse</p> </li> </ul> <p>These benefits often result in significant cost reduction.</p> <p>The connector implements most push down interfaces defined by DataSource V2, such as <code>SupportsPushDownLimit</code>, <code>SupportsPushDownFilters</code>, <code>SupportsPushDownAggregates</code>, <code>SupportsPushDownRequiredColumns</code>.</p> <p>The below example shows how <code>SupportsPushDownAggregates</code> and <code>SupportsPushDownRequiredColumns</code> work. </p> <p> </p> Push Down disabled <p> </p> Push Down enabled"},{"location":"internals/02_read/#bucket-join","title":"Bucket Join","text":"<p>Sort merge join is a general solution for two large table inner join, it requires two table shuffle by join key first, then do local sort by join key in each data partition, finally do stream-stream like look up to get the final result.</p> <p>In some cases, the tables store collocated by join keys, w/  Storage-Partitioned Join(or V2 Bucket Join), Spark could leverage the existing ClickHouse table layout to eliminate the expensive shuffle and sort operations.</p> <p> </p> Sort Merge Join <p> </p> Bucket Join"},{"location":"internals/03_write/","title":"How writing of the connector works?","text":"<p>As we know, the ClickHouse <code>MergeTree</code> is a LSM-like format, it's not optimized for frequent and random record insertion, batch append operation is recommended for large amount of data ingestion.</p> <p>So, to achieve better performance, we should re-organize the <code>DataFrame</code> to fit ClickHouse data layout before inserting.</p> <p>SPARK-23889 allows data source connector to expose sorting and clustering requirements of <code>DataFrame</code> before writing. By default, for <code>Distributed</code> table, this connector requires the <code>DataFrame</code> clustered by <code>[sharding keys, partition keys]</code> and sorted by <code>[sharding keys, partition keys, ordering keys]</code>; for normal <code>*MergeTree</code> table, this connector requires the <code>DataFrame</code> sorted by <code>[partition keys, ordering keys]</code> and sorted by <code>[partition keys, ordering keys]</code>.</p> <p>Warning</p> <p>Limitation: Spark does NOT support expressions in sharding keys and partition keys w/o SPARK-39607.</p> <p></p> <p>In some cases, the strict data distribution requirements may lead small parallelism and data skew, and finally cause bad performance. SPARK-37523(requires Spark 3.4+) is introduced to allow relaxing the data distribution requirements to overcome those shortages.</p> <p>Also, you can consider disabling some configurations like <code>spark.clickhouse.write.repartitionByPartition</code> to avoid such performance degradation.</p>"},{"location":"quick_start/01_get_the_library/","title":"Get the Library","text":""},{"location":"quick_start/01_get_the_library/#download-the-library","title":"Download the Library","text":"<p>The name pattern of binary jar is </p> <pre><code>clickhouse-spark-runtime-${spark_binary_version}_${scala_binary_version}-${version}.jar\n</code></pre> <p>you can find all available released jars under Maven Central Repository and all daily build SNAPSHOT jars under Sonatype OSS Snapshots Repository.</p>"},{"location":"quick_start/01_get_the_library/#import-as-dependency","title":"Import as Dependency","text":""},{"location":"quick_start/01_get_the_library/#gradle","title":"Gradle","text":"<pre><code>dependencies {\n  implementation(\"com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3\")\n  implementation(\"com.clickhouse:clickhouse-jdbc:0.4.6:all\") { transitive = false }\n}\n</code></pre> <p>Add the following repository if you want to use SNAPSHOT version. </p> <pre><code>repositries {\n  maven { url = \"https://oss.sonatype.org/content/repositories/snapshots\" }\n}\n</code></pre>"},{"location":"quick_start/01_get_the_library/#maven","title":"Maven","text":"<pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;com.github.housepower&lt;/groupId&gt;\n  &lt;artifactId&gt;clickhouse-spark-runtime-3.4_2.12&lt;/artifactId&gt;\n  &lt;version&gt;0.7.3&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;com.clickhouse&lt;/groupId&gt;\n  &lt;artifactId&gt;clickhouse-jdbc&lt;/artifactId&gt;\n  &lt;classifier&gt;all&lt;/classifier&gt;\n  &lt;version&gt;0.4.6&lt;/version&gt;\n  &lt;exclusions&gt;\n    &lt;exclusion&gt;\n      &lt;groupId&gt;*&lt;/groupId&gt;\n      &lt;artifactId&gt;*&lt;/artifactId&gt;\n    &lt;/exclusion&gt;\n  &lt;/exclusions&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Add the following repository if you want to use SNAPSHOT version.</p> <pre><code>&lt;repositories&gt;\n  &lt;repository&gt;\n    &lt;id&gt;sonatype-oss-snapshots&lt;/id&gt;\n    &lt;name&gt;Sonatype OSS Snapshots Repository&lt;/name&gt;\n    &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt;\n  &lt;/repository&gt;\n&lt;/repositories&gt;\n</code></pre>"},{"location":"quick_start/02_play_with_spark_sql/","title":"Play with Spark SQL","text":"<p>Note: For SQL-only use cases, Apache Kyuubi is recommended for Production.</p>"},{"location":"quick_start/02_play_with_spark_sql/#launch-spark-sql-cli","title":"Launch Spark SQL CLI","text":"<pre><code>$SPARK_HOME/bin/spark-sql \\\n  --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\\n  --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\\n  --conf spark.sql.catalog.clickhouse.protocol=http \\\n  --conf spark.sql.catalog.clickhouse.http_port=${CLICKHOUSE_HTTP_PORT:-8123} \\\n  --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\\n  --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\\n  --conf spark.sql.catalog.clickhouse.database=default \\\n  --jars /path/clickhouse-spark-runtime-3.4_2.12:0.7.3.jar,/path/clickhouse-jdbc-0.4.6-all.jar\n</code></pre> <p>The following argument</p> <pre><code>  --jars /path/clickhouse-spark-runtime-3.4_2.12:0.7.3.jar,/path/clickhouse-jdbc-0.4.6-all.jar\n</code></pre> <p>can be replaced by</p> <pre><code>  --repositories https://{maven-cental-mirror or private-nexus-repo} \\\n  --packages com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3,com.clickhouse:clickhouse-jdbc:0.4.6:all\n</code></pre> <p>to avoid copying jar to your Spark client node.</p>"},{"location":"quick_start/02_play_with_spark_sql/#operations","title":"Operations","text":"<p>Basic operations, e.g. create database, create table, write table, read table, etc.</p> <pre><code>spark-sql&gt; use clickhouse;\nTime taken: 0.016 seconds\n\nspark-sql&gt; create database if not exists test_db;\nTime taken: 0.022 seconds\n\nspark-sql&gt; show databases;\ndefault\nsystem\ntest_db\nTime taken: 0.289 seconds, Fetched 3 row(s)\n\nspark-sql&gt; CREATE TABLE test_db.tbl_sql (\n         &gt;   create_time TIMESTAMP NOT NULL,\n         &gt;   m           INT       NOT NULL COMMENT 'part key',\n         &gt;   id          BIGINT    NOT NULL COMMENT 'sort key',\n         &gt;   value       STRING\n         &gt; ) USING ClickHouse\n         &gt; PARTITIONED BY (m)\n         &gt; TBLPROPERTIES (\n         &gt;   engine = 'MergeTree()',\n         &gt;   order_by = 'id',\n         &gt;   settings.index_granularity = 8192\n         &gt; );\nTime taken: 0.242 seconds\n\nspark-sql&gt; insert into test_db.tbl_sql values\n         &gt; (timestamp'2021-01-01 10:10:10', 1, 1L, '1'),\n         &gt; (timestamp'2022-02-02 10:10:10', 2, 2L, '2')\n         &gt; as tabl(create_time, m, id, value);\nTime taken: 0.276 seconds\n\nspark-sql&gt; select * from test_db.tbl_sql;\n2021-01-01 10:10:10 1   1   1\n2022-02-02 10:10:10 2   2   2\nTime taken: 0.116 seconds, Fetched 2 row(s)\n\nspark-sql&gt; insert into test_db.tbl_sql select * from test_db.tbl_sql;\nTime taken: 1.028 seconds\n\nspark-sql&gt; insert into test_db.tbl_sql select * from test_db.tbl_sql;\nTime taken: 0.462 seconds\n\nspark-sql&gt; select count(*) from test_db.tbl_sql;\n6\nTime taken: 1.421 seconds, Fetched 1 row(s)\n\nspark-sql&gt; select * from test_db.tbl_sql;\n2021-01-01 10:10:10 1   1   1\n2021-01-01 10:10:10 1   1   1\n2021-01-01 10:10:10 1   1   1\n2022-02-02 10:10:10 2   2   2\n2022-02-02 10:10:10 2   2   2\n2022-02-02 10:10:10 2   2   2\nTime taken: 0.123 seconds, Fetched 6 row(s)\n\nspark-sql&gt; delete from test_db.tbl_sql where id = 1;\nTime taken: 0.129 seconds\n\nspark-sql&gt; select * from test_db.tbl_sql;\n2022-02-02 10:10:10 2   2   2\n2022-02-02 10:10:10 2   2   2\n2022-02-02 10:10:10 2   2   2\nTime taken: 0.101 seconds, Fetched 3 row(s)\n</code></pre>"},{"location":"quick_start/03_play_with_spark_shell/","title":"Play with Spark Shell","text":""},{"location":"quick_start/03_play_with_spark_shell/#launch-spark-shell","title":"Launch Spark Shell","text":"<pre><code>$SPARK_HOME/bin/spark-shell \\\n  --conf spark.sql.catalog.clickhouse=xenon.clickhouse.ClickHouseCatalog \\\n  --conf spark.sql.catalog.clickhouse.host=${CLICKHOUSE_HOST:-127.0.0.1} \\\n  --conf spark.sql.catalog.clickhouse.protocol=http \\\n  --conf spark.sql.catalog.clickhouse.http_port=${CLICKHOUSE_HTTP_PORT:-8123} \\\n  --conf spark.sql.catalog.clickhouse.user=${CLICKHOUSE_USER:-default} \\\n  --conf spark.sql.catalog.clickhouse.password=${CLICKHOUSE_PASSWORD:-} \\\n  --conf spark.sql.catalog.clickhouse.database=default \\\n  --jars /path/clickhouse-spark-runtime-3.4_2.12:0.7.3.jar,/path/clickhouse-jdbc-0.4.6-all.jar\n</code></pre> <p>The following argument</p> <pre><code>  --jars /path/clickhouse-spark-runtime-3.4_2.12:0.7.3.jar,/path/clickhouse-jdbc-0.4.6-all.jar\n</code></pre> <p>can be replaced by</p> <pre><code>  --repositories https://{maven-cental-mirror or private-nexus-repo} \\\n  --packages com.github.housepower:clickhouse-spark-runtime-3.4_2.12:0.7.3,com.clickhouse:clickhouse-jdbc:0.4.6:all\n</code></pre> <p>to avoid copying jar to your Spark client node.</p>"},{"location":"quick_start/03_play_with_spark_shell/#operations","title":"Operations","text":"<p>Basic operations, e.g. create database, create table, write table, read table, etc.</p> <pre><code>scala&gt; spark.sql(\"use clickhouse\")\nres0: org.apache.spark.sql.DataFrame = []\n\nscala&gt; spark.sql(\"create database test_db\")\nres1: org.apache.spark.sql.DataFrame = []\n\nscala&gt; spark.sql(\"show databases\").show\n+---------+\n|namespace|\n+---------+\n|  default|\n|   system|\n|  test_db|\n+---------+\n\nscala&gt; spark.sql(\"\"\"\n     | CREATE TABLE test_db.tbl (\n     |   create_time TIMESTAMP NOT NULL,\n     |   m           INT       NOT NULL COMMENT 'part key',\n     |   id          BIGINT    NOT NULL COMMENT 'sort key',\n     |   value       STRING\n     | ) USING ClickHouse\n     | PARTITIONED BY (m)\n     | TBLPROPERTIES (\n     |   engine = 'MergeTree()',\n     |   order_by = 'id',\n     |   settings.index_granularity = 8192\n     | )\n     | \"\"\")\nres2: org.apache.spark.sql.DataFrame = []\n\nscala&gt; :paste\n// Entering paste mode (ctrl-D to finish)\n\nspark.createDataFrame(Seq(\n    (\"2021-01-01 10:10:10\", 1L, \"1\"),\n    (\"2022-02-02 10:10:10\", 2L, \"2\")\n)).toDF(\"create_time\", \"id\", \"value\")\n    .withColumn(\"create_time\", to_timestamp($\"create_time\"))\n    .withColumn(\"m\", month($\"create_time\"))\n    .select($\"create_time\", $\"m\", $\"id\", $\"value\")\n    .writeTo(\"test_db.tbl\")\n    .append\n\n// Exiting paste mode, now interpreting.\n\nscala&gt; spark.table(\"test_db.tbl\").show\n+-------------------+---+---+-----+\n|        create_time|  m| id|value|\n+-------------------+---+---+-----+\n|2021-01-01 10:10:10|  1|  1|    1|\n|2022-02-02 10:10:10|  2|  2|    2|\n+-------------------+---+---+-----+\n\nscala&gt; spark.sql(\"DELETE FROM test_db.tbl WHERE id=1\")\nres3: org.apache.spark.sql.DataFrame = []\n\nscala&gt; spark.table(\"test_db.tbl\").show\n+-------------------+---+---+-----+\n|        create_time|  m| id|value|\n+-------------------+---+---+-----+\n|2022-02-02 10:10:10|  2|  2|    2|\n+-------------------+---+---+-----+\n</code></pre> <p>Execute ClickHouse native SQL.</p> <pre><code>scala&gt; val options = Map(\n     |     \"host\" -&gt; \"clickhouse\",\n     |     \"protocol\" -&gt; \"http\",\n     |     \"http_port\" -&gt; \"8123\",\n     |     \"user\" -&gt; \"default\",\n     |     \"password\" -&gt; \"\"\n     | )\n\nscala&gt; val sql = \"\"\"\n     | |CREATE TABLE test_db.person (\n     | |  id    Int64,\n     | |  name  String,\n     | |  age Nullable(Int32)\n     | |)\n     | |ENGINE = MergeTree()\n     | |ORDER BY id\n     | \"\"\".stripMargin\n\nscala&gt; spark.executeCommand(\"xenon.clickhouse.ClickHouseCommandRunner\", sql, options) \n\nscala&gt; spark.sql(\"show tables in clickhouse_s1r1.test_db\").show\n+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n|  test_db|   person|      false|\n+---------+---------+-----------+\n\nscala&gt; spark.table(\"clickhouse_s1r1.test_db.person\").printSchema\nroot\n |-- id: long (nullable = false)\n |-- name: string (nullable = false)\n |-- age: integer (nullable = true)\n</code></pre>"}]}